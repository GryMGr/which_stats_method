[
["index.html", "Dashboards project Preface", " Dashboards project Richard White 2018-10-31 Preface The dashboards project is a project at FHI concerned with running automated analyses on data. An automated analysis is any analysis that: Will be repeated multiple times in the future Always has an input dataset with consistent file structure Always has the same expected output (e.g. tables, graphs, reports) "],
["introduction.html", "1 Introduction 1.1 Executive summary 1.2 What is an automated analysis? 1.3 Why not have one project for each automated analysis? 1.4 Important repositories", " 1 Introduction 1.1 Executive summary The dashboards project is a project at FHI concerned with running automated analyses on data. In principle, the dashboards project is split up into three parts: The umbrella infrastructure (i.e. Docker containers, continuous integration, chron jobs, etc.) The R package for each automated analysis Integrating the R package into the physical system (e.g. specifying where the data is) 1.2 What is an automated analysis? An automated analysis is any analysis that: Will be repeated multiple times in the future Always has an input dataset with consistent file structure Always has the same expected output (e.g. tables, graphs, reports) 1.3 Why not have one project for each automated analysis? Automated analyses have a lot of code and infrastructure in common. Automated analyses: Need their code to be tested via unit testing to ensure the results are correct Need their code to be tested via integration testing to ensure everything runs Need to be run at certain times Need to be able to send emails notifying people that the analyses have finished running Need to make their results accessible to the relevant people By combining them all in one umbrella project we can force everyone to use the same infrastructure and coding principles, so we: Only need to solve a problem once Only need to maintain one system Can easily work on multiple projects, as we all speak the same language 1.4 Important repositories 1.4.1 Infrastructure https://github.com/raubreywhite/dashboards_control/ (private) This contains the Docker files, cronfiles, all bash scripts, etc. https://folkehelseinstituttet.github.io/dashboards/ (this one) This contains the R executable for each automated analysis. https://folkehelseinstituttet.github.io/fhi/ This is an R package that contains helper functions. 1.4.2 Automated analyses R packages https://folkehelseinstituttet.github.io/dashboards_sykdomspuls/ https://folkehelseinstituttet.github.io/dashboards_normomo/ https://folkehelseinstituttet.github.io/dashboards_sykdomspuls_pdf/ https://folkehelseinstituttet.github.io/dashboards_sykdomspuls_log/ "],
["integrating-the-r-package-into-the-physical-system.html", "2 Integrating the R package into the physical system 2.1 Summary 2.2 RunProcess.R 2.3 0_run.sh 2.4 RunTest.R", " 2 Integrating the R package into the physical system 2.1 Summary An R package is not enough to run an analysis – something needs to physically call the functions inside the R package. That is, the R package needs to be integrated into the physical system. Everything related to integrating the R package into the physical system lives in the dashboards repository. Inside the dashboards repository we have: - dev/ |-- src/ |-- sykdomspuls/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- normomo/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- sykdomspuls_log/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- sykdomspuls_pdf/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R 2.2 RunProcess.R 2.2.1 Aim An automated analysis needs to: Know the location of the data/results folders. Check for new data in these folders. If no new data - then quit. Load in the data. Load in the analysis functions. Run the analyses. Save the results. RunProcess.R is responsible for these tasks. We can think of it as an extremely short and extremely high-level script that implements the analysis scripts. Depending on the automated analysis RunProcess.R can be run every two minutes (constantly checking for new data), or once a week (when we know that data will only be available on a certain day/time). 2.2.2 Bounded context Only one instance of RunProcess.R can be run at a time. Data only exists on physical folders on the system. The following folder structure exists on the system (here the name of the automated analysis is ANALYSIS): /data_raw/ |-- ANALYSIS/ /data_clean/ |-- ANALYSIS/ /data_app/ |-- ANALYSIS/ /results/ |-- ANALYSIS/ /src/ |-- ANALYSIS/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R Point #1 is important because if RunProcess.R is run every 2 minutes (constantly checking for new data) but the analyses take 3 hours to run, then we need to ensure that only one instance of RunProcess.R can be run at a time. Point #2 is important because sometimes: Data files need to be downloaded from external SFTP servers (normomo, sykdomspulslog). Results files need to be uploaded to external SFTP servers (sykdomspuls). If we include code to download/upload the files from SFTP servers inside RunProcess.R then it makes it very difficult to test RunProcess.R (because we will then need to simulate SFTP servers inside our testing infrastructure). If we know that RunProcess.R only accesses files that are available on physical folders in the system, then our testing infrastructure is a lot easier to create and maintain. 2.3 0_run.sh 2.3.1 Aim The aim of 0_run.sh is to ensure that: Points 1 and 2 of the bounded context of RunProcess.R happen Run RunProcess.R With regards to the bounded context, we ensure that only one instance of RunProcess.R is run at a time through the use of flock. (If neccessary) with regards to the bounded context, we use sshpass, sftp, and ncftpput to download/upload files from SFTP servers. We then run RunProcess.R with a standard call: /usr/local/bin/Rscript /src/ANALYSIS/RunProcess.R 2.4 RunTest.R 2.4.1 Aim The aim of RunTest.R is to perform integration testing on the automated analysis. This integration testing is performed as part of the Jenkins build pipeline. "],
["r-packages.html", "3 R packages 3.1 Summary 3.2 Requirements 3.3 Deployment via travis-ci and drat", " 3 R packages 3.1 Summary Each automated analysis has its own R package: sykdomspuls normomo sykdomspulspdf sykdomspulslog Each R package contains all of the code necessary for that automated analysis. Typical examples are: Data cleaning Signal analysis Graph generation Report generation 3.2 Requirements The R packages should be developed using unit testing as implemented in the testthat package. Furthermore, the R package should operate (and be able to be tested) independently from the real datasets on the system. This is because the real datasets cannot be shared publically or uploaded to github. To circumvent this issue, each package will need to develop functions that can generate fake data. GenFakeDataRaw is one example from sykdomspuls. We also require that unit tests are created to test the formatting/structure of results. ValidateAnalysisResults is one example from sykdomspuls, where the names of the data.table are checked against reference values to ensure that the structure of the results are not accidentally changed. 3.3 Deployment via travis-ci and drat Unit testing is then automatically run using travis-ci. If the R package passes all tests, then we use drat to deploy a built version of the package to Folkehelseinstituttet’s R repository: https://folkehelseinstituttet.github.io/drat/. "],
["umbrella-infrastructure.html", "4 Umbrella Infrastructure 4.1 Executive summary 4.2 What is an automated analysis? 4.3 Why not have one project for each automated analysis?", " 4 Umbrella Infrastructure Three computers 4.1 Executive summary The dashboards project is a project at FHI concerned with running automated analyses on data. In principle, the dashboards project is split up into three parts: The overarching infrastructure (i.e. Docker containers, continuous integration, chron jobs, etc.) The R package for each automated analysis The executable for each automated analysis 4.2 What is an automated analysis? An automated analysis is any analysis that: Will be repeated multiple times in the future Always has an input dataset with consistent file structure Always has the same expected output (e.g. tables, graphs, reports) 4.3 Why not have one project for each automated analysis? Automated analyses have a lot of code and infrastructure in common. Automated analyses: Need their code to be tested via unit testing to ensure the results are correct Need their code to be tested via integration testing to ensure everything runs Need to be run at certain times Need to be able to send emails notifying people that the analyses have finished running Need to make their results accessible to the relevant people By combining them all in one umbrella project we can force everyone to use the same infrastructure, so we: Only need to solve a problem once Only need to maintain one system Can easily work on multiple projects, as we all speak the same language "],
["contributing.html", "5 Contributing 5.1 Development guidelines 5.2 Code style", " 5 Contributing 5.1 Development guidelines We try to follow the GitHub flow for development. Fork [this repo][repo] and clone it to your computer. To learn more about this process, see this guide. Add the Folkehelseinstituttet repository as your upstream: git remote add upstream https://github.com/folkehelseinstituttet/ORIGINAL_REPOSITORY.git If you have forked and cloned the project before and it has been a while since you worked on it, merge changes from the original repo to your clone by using: git fetch upstream git merge upstream/master Open the RStudio project file (.Rproj). Make your changes: Write your code. Test your code (bonus points for adding unit tests). Document your code (see function documentation above). Do an R CMD check using devtools::check() and aim for 0 errors and warnings. Commit your changes locally Merge changes from the original repo (again) Do an R CMD check using devtools::check() and aim for 0 errors and warnings. Commit and push your changes. Submit a pull request. If you are reviewing the pull request, wait until the travis-ci unit tests have finished Please make sure that the unit tests PASS before merging in!! 5.2 Code style Function names start with capital letters Variable names start with small letters Environments should be in ALL CAPS Reference Hadley’s style code &lt;- is preferred over = for assignment Indentation is with two spaces, not two or a tab. There should be no tabs in code files. if () {} else {} constructions should always use full curly braces even when usage seems unnecessary from a clarity perspective. TODO statements should be opened as GitHub issues with links to specific code files and code lines, rather than written inline. Follow Hadley’s suggestion for aligning long functions with many arguments: long_function_name &lt;- function(a = &quot;a long argument&quot;, b = &quot;another argument&quot;, c = &quot;another long argument&quot;) { # As usual code is indented by two spaces. } Never use print() to send text to the console. Instead use message(), warning(), and error() as appropriate. Use environment variables, not options(), to store global arguments that are used by many or all functions. "]
]
